{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets\n",
        "!pip install evaluate"
      ],
      "metadata": {
        "id": "59-hlwIqlcmY",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CMA3sRZpkjP7"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from datasets import Dataset, DatasetDict, load_dataset, interleave_datasets, load_from_disk\n",
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, GenerationConfig, TrainingArguments, Trainer\n",
        "import torch\n",
        "import time\n",
        "import evaluate\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.is_available()"
      ],
      "metadata": {
        "id": "x7smKCJvk-Bh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name='t5-small'\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "original_model = AutoModelForSeq2SeqLM.from_pretrained(model_name, torch_dtype=torch.bfloat16)\n",
        "original_model = original_model.to('cuda')"
      ],
      "metadata": {
        "id": "bdCBaKaWopKT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**LOAD DATASET**"
      ],
      "metadata": {
        "id": "L8e1TassF4PP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "id": "Op7IdWV3o06L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"qa.csv\")\n",
        "print(df.head())\n",
        "print(df.columns)\n"
      ],
      "metadata": {
        "id": "3BdxzLyvo39B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset, DatasetDict\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# 1. Load CSV\n",
        "df = pd.read_csv(\"qa.csv\")\n",
        "\n",
        "# 2. Optional: Strip whitespace dari kolom\n",
        "df[\"question\"] = df[\"question\"].str.strip()\n",
        "df[\"context\"] = df[\"context\"].str.strip()\n",
        "df[\"answer\"] = df[\"answer\"].str.strip()\n",
        "\n",
        "# 3. Split data: 80% train, 10% validation, 10% test\n",
        "train_df, temp_df = train_test_split(df, test_size=0.2, random_state=42)\n",
        "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
        "\n",
        "# 4. Convert ke HuggingFace Dataset\n",
        "dataset = DatasetDict({\n",
        "    \"train\": Dataset.from_pandas(train_df),\n",
        "    \"validation\": Dataset.from_pandas(val_df),\n",
        "    \"test\": Dataset.from_pandas(test_df)\n",
        "})\n",
        "\n",
        "# 5. Save ke disk (opsional, supaya bisa di-load lagi nanti)\n",
        "dataset.save_to_disk(\"qa_dataset\")\n",
        "\n",
        "# 6. Print summary\n",
        "print(dataset)\n"
      ],
      "metadata": {
        "id": "v-UzM-RhqEMv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import DatasetDict\n",
        "\n",
        "# Misalnya dataset kamu bernama `dataset`\n",
        "dataset = DatasetDict({\n",
        "    'train': dataset['train'].remove_columns(['__index_level_0__']),\n",
        "    'validation': dataset['validation'].remove_columns(['__index_level_0__']),\n",
        "    'test': dataset['test'].remove_columns(['__index_level_0__']),\n",
        "})\n"
      ],
      "metadata": {
        "id": "wdtpxvVCrkn4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset)\n"
      ],
      "metadata": {
        "id": "h74LjAZIrnrJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.save_to_disk(\"qa_dataset_cleaneddd\")"
      ],
      "metadata": {
        "id": "bOtSUUMcrrPW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset['test'][0]"
      ],
      "metadata": {
        "id": "OzXMogK1rxf1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Preprocess the Datasets**"
      ],
      "metadata": {
        "id": "snEubt2PGMo-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_function(example):\n",
        "\n",
        "#     print(len(example[\"question\"]))\n",
        "    start_prompt = \"Tables:\\n\"\n",
        "    middle_prompt = \"\\n\\nQuestion:\\n\"\n",
        "    end_prompt = \"\\n\\nAnswer:\\n\"\n",
        "\n",
        "    data_zip = zip(example['context'], example['question'])\n",
        "    prompt = [start_prompt + context + middle_prompt + question + end_prompt for context, question in data_zip]\n",
        "    example['input_ids'] = tokenizer(prompt, padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids\n",
        "    example['labels'] = tokenizer(example['answer'], padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids\n",
        "#     print(prompt[0])\n",
        "#     print()\n",
        "\n",
        "    return example\n",
        "\n",
        "# The dataset actually contains 3 diff splits: train, validation, test.\n",
        "# The tokenize_function code is handling all data across all splits in batches.\n",
        "\n",
        "try:\n",
        "    tokenized_datasets = load_from_disk(\"tokenized_datasets\")\n",
        "    print(\"Loaded Tokenized Dataset\")\n",
        "except:\n",
        "    tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
        "    tokenized_datasets = tokenized_datasets.remove_columns(['question', 'context', 'answer'])\n",
        "\n",
        "    tokenized_datasets.save_to_disk(\"tokenized_datasets\")\n",
        "    print(\"Tokenized and Saved Dataset\")"
      ],
      "metadata": {
        "id": "_vzVpc6zsJjG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenized_datasets.keys())\n",
        "print(tokenized_datasets['train'][0].keys())\n",
        "print(tokenized_datasets['train'][0]['input_ids'][:10])\n",
        "print(tokenized_datasets['train'][0]['labels'][:10])\n",
        "print(tokenized_datasets)"
      ],
      "metadata": {
        "id": "7nQPvqezsPny"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Shapes of the datasets:\")\n",
        "print(f\"Training: {tokenized_datasets['train'].shape}\")\n",
        "print(f\"Validation: {tokenized_datasets['validation'].shape}\")\n",
        "print(f\"Test: {tokenized_datasets['test'].shape}\")\n",
        "\n",
        "print(tokenized_datasets)"
      ],
      "metadata": {
        "id": "yVUIAdF5sTcu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Test the Model with Zero Shot Inferencing**"
      ],
      "metadata": {
        "id": "yRL-yYD_GVdU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "index = 0\n",
        "\n",
        "question = dataset['test'][index]['question']\n",
        "context = dataset['test'][index]['context']\n",
        "answer = dataset['test'][index]['answer']\n",
        "\n",
        "prompt = f\"\"\"Tables:\n",
        "{context}\n",
        "\n",
        "Question:\n",
        "{question}\n",
        "\n",
        "Answer:\n",
        "\"\"\"\n",
        "\n",
        "inputs = tokenizer(prompt, return_tensors='pt')\n",
        "inputs = inputs.to('cuda')\n",
        "\n",
        "output = tokenizer.decode(\n",
        "    original_model.generate(\n",
        "        inputs[\"input_ids\"],\n",
        "        max_new_tokens=200,\n",
        "    )[0],\n",
        "    skip_special_tokens=True\n",
        ")\n",
        "\n",
        "dash_line = '-'.join('' for x in range(100))\n",
        "print(dash_line)\n",
        "print(f'INPUT PROMPT:\\n{prompt}')\n",
        "print(dash_line)\n",
        "print(f'BASELINE HUMAN ANSWER:\\n{answer}\\n')\n",
        "print(dash_line)\n",
        "print(f'MODEL GENERATION - ZERO SHOT:\\n{output}')"
      ],
      "metadata": {
        "id": "u2lr-g8RsWRd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Perform Full Fine-Tuning**"
      ],
      "metadata": {
        "id": "e0qOKVSTGgWb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    finetuned_model = AutoModelForSeq2SeqLM.from_pretrained(\"finetuned_model_2_epoch\")\n",
        "    finetuned_model = finetuned_model.to('cuda')\n",
        "    to_train = False\n",
        "\n",
        "except:\n",
        "    to_train = True\n",
        "    finetuned_model = AutoModelForSeq2SeqLM.from_pretrained(model_name, torch_dtype=torch.bfloat16)\n",
        "    finetuned_model = finetuned_model.to('cuda')\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "IbUZx-Zhs6uy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "if to_train:\n",
        "    output_dir = f'./sql-training-{str(int(time.time()))}'\n",
        "\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=output_dir,\n",
        "        learning_rate=5e-3,\n",
        "        num_train_epochs=2,\n",
        "        per_device_train_batch_size=16,     # batch size per device during training\n",
        "        per_device_eval_batch_size=16,      # batch size for evaluation\n",
        "        weight_decay=0.01,\n",
        "        logging_steps=50,\n",
        "        eval_strategy='steps',        # evaluation strategy to adopt during training\n",
        "        eval_steps=500,                     # number of steps between evaluation\n",
        "    )\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=finetuned_model,\n",
        "        args=training_args,\n",
        "        train_dataset=tokenized_datasets['train'],\n",
        "        eval_dataset=tokenized_datasets['validation'],\n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "\n",
        "    finetuned_model.save_pretrained(\"finetuned_model_2_epoch\")"
      ],
      "metadata": {
        "id": "kER9WYoHtE39"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "finetuned_model = AutoModelForSeq2SeqLM.from_pretrained(\"finetuned_model_2_epoch\")\n",
        "finetuned_model = finetuned_model.to('cuda')"
      ],
      "metadata": {
        "id": "0g9tOO6-zQLX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Test the Fine Tuned Model with Zero Shot Inferencing**"
      ],
      "metadata": {
        "id": "e1Gmw4bKHE8V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "index = 0\n",
        "# index = len(dataset['test'])-200\n",
        "\n",
        "question = dataset['test'][index]['question']\n",
        "context = dataset['test'][index]['context']\n",
        "answer = dataset['test'][index]['answer']\n",
        "\n",
        "prompt = f\"\"\"Tables:\n",
        "{context}\n",
        "\n",
        "Question:\n",
        "{question}\n",
        "\n",
        "Answer:\n",
        "\"\"\"\n",
        "\n",
        "inputs = tokenizer(prompt, return_tensors='pt')\n",
        "inputs = inputs.to('cuda')\n",
        "\n",
        "output = tokenizer.decode(\n",
        "    finetuned_model.generate(\n",
        "        inputs[\"input_ids\"],\n",
        "        max_new_tokens=200,\n",
        "    )[0],\n",
        "    skip_special_tokens=True\n",
        ")\n",
        "\n",
        "dash_line = '-'.join('' for x in range(100))\n",
        "print(dash_line)\n",
        "print(f'INPUT PROMPT:\\n{prompt}')\n",
        "print(dash_line)\n",
        "print(f'BASELINE HUMAN ANSWER:\\n{answer}\\n')\n",
        "print(dash_line)\n",
        "print(f'FINE-TUNED MODEL - ZERO SHOT:\\n{output}')\n"
      ],
      "metadata": {
        "id": "OKiHXCFGzkou"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Evaluate the Model Quantitatively (with ROUGE Metric)**"
      ],
      "metadata": {
        "id": "W203tpL2HN2t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform inferences for test dataset. Do 25 only, due to time it takes.\n",
        "\n",
        "questions = dataset['test'][0:25]['question']\n",
        "contexts = dataset['test'][0:25]['context']\n",
        "human_baseline_answers = dataset['test'][0:25]['answer']\n",
        "\n",
        "original_model_answers = []\n",
        "finetuned_model_answers = []\n",
        "\n",
        "for idx, question in enumerate(questions):\n",
        "\n",
        "    prompt = f\"\"\"Tables:\n",
        "{contexts[idx]}\n",
        "\n",
        "Question:\n",
        "{question}\n",
        "\n",
        "Answer:\n",
        "\"\"\"\n",
        "\n",
        "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
        "    input_ids = input_ids.to('cuda')\n",
        "\n",
        "    human_baseline_text_output = human_baseline_answers[idx]\n",
        "\n",
        "    original_model_outputs = original_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=300))\n",
        "    original_model_text_output = tokenizer.decode(original_model_outputs[0], skip_special_tokens=True)\n",
        "    original_model_answers.append(original_model_text_output)\n",
        "\n",
        "    finetuned_model_outputs = finetuned_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=300))\n",
        "    finetuned_model_text_output = tokenizer.decode(finetuned_model_outputs[0], skip_special_tokens=True)\n",
        "    finetuned_model_answers.append(finetuned_model_text_output)\n",
        "\n",
        "zipped_summaries = list(zip(human_baseline_answers, original_model_answers, finetuned_model_answers))\n",
        "\n",
        "df = pd.DataFrame(zipped_summaries, columns = ['human_baseline_answers', 'original_model_answers', 'finetuned_model_answers'])\n",
        "# df"
      ],
      "metadata": {
        "id": "GUit-ybBz0mI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rouge_score\n"
      ],
      "metadata": {
        "id": "3TlmzBNL0wLe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rouge = evaluate.load('rouge')\n",
        "\n",
        "original_model_results = rouge.compute(\n",
        "    predictions=original_model_answers,\n",
        "    references=human_baseline_answers[0:len(original_model_answers)],\n",
        "    use_aggregator=True,\n",
        "    use_stemmer=True,\n",
        ")\n",
        "print('ORIGINAL MODEL:')\n",
        "print(original_model_results)\n",
        "\n",
        "\n",
        "finetuned_model_results = rouge.compute(\n",
        "    predictions=finetuned_model_answers,\n",
        "    references=human_baseline_answers[0:len(finetuned_model_answers)],\n",
        "    use_aggregator=True,\n",
        "    use_stemmer=True,\n",
        ")\n",
        "print('FINE-TUNED MODEL:')\n",
        "print(finetuned_model_results)"
      ],
      "metadata": {
        "id": "tfS-TXFJ0gVe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_metric = evaluate.load(\"accuracy\")\n",
        "f1_metric = evaluate.load(\"f1\")\n",
        "recall_metric = evaluate.load(\"recall\")"
      ],
      "metadata": {
        "id": "uNLC4fZi08Dy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score, recall_score\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "import string, re\n",
        "\n",
        "# ====================\n",
        "# FUNGSI NORMALISASI TEKS\n",
        "# ====================\n",
        "def normalize_text(s):\n",
        "    def remove_articles(text):\n",
        "        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
        "    def white_space_fix(text):\n",
        "        return ' '.join(text.split())\n",
        "    def remove_punc(text):\n",
        "        return ''.join(ch for ch in text if ch not in set(string.punctuation))\n",
        "    def lower(text):\n",
        "        return text.lower()\n",
        "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
        "\n",
        "# ====================\n",
        "# METRIK MANUAL\n",
        "# ====================\n",
        "def compute_exact_match(pred, truth):\n",
        "    return int(normalize_text(pred) == normalize_text(truth))\n",
        "\n",
        "def compute_f1(pred, truth):\n",
        "    pred_tokens = normalize_text(pred).split()\n",
        "    truth_tokens = normalize_text(truth).split()\n",
        "    common = set(pred_tokens) & set(truth_tokens)\n",
        "    if not common:\n",
        "        return 0.0\n",
        "    precision = len(common) / len(pred_tokens)\n",
        "    recall = len(common) / len(truth_tokens)\n",
        "    return 2 * (precision * recall) / (precision + recall)\n",
        "\n",
        "def compute_bleu(pred, truth):\n",
        "    pred_tokens = pred.lower().split()\n",
        "    truth_tokens = truth.lower().split()\n",
        "    smoothie = SmoothingFunction().method4\n",
        "    return sentence_bleu([truth_tokens], pred_tokens, smoothing_function=smoothie)\n",
        "\n",
        "def compute_execution_match(pred, truth):\n",
        "    try:\n",
        "        return int(eval(pred) == eval(truth))\n",
        "    except:\n",
        "        return 0\n",
        "\n",
        "# ====================\n",
        "# CONTOH JAWABAN\n",
        "# ====================\n",
        "finetuned_model_answers = [\"3 + 5\", \"the capital is paris\", \"no\", \"1 / 0\", \"yes\"]\n",
        "human_baseline_answers = [\"8\", \"paris is the capital\", \"no\", \"error\", \"yes\"]\n",
        "\n",
        "# ====================\n",
        "# EVALUASI\n",
        "# ====================\n",
        "em_scores = []\n",
        "f1_scores = []\n",
        "bleu_scores = []\n",
        "execution_scores = []\n",
        "true_labels = [1] * len(human_baseline_answers)\n",
        "predicted_labels = []\n",
        "\n",
        "for pred, truth in zip(finetuned_model_answers, human_baseline_answers):\n",
        "    em = compute_exact_match(pred, truth)\n",
        "    f1 = compute_f1(pred, truth)\n",
        "    bleu = compute_bleu(pred, truth)\n",
        "    exec_match = compute_execution_match(pred, truth)\n",
        "\n",
        "    em_scores.append(em)\n",
        "    f1_scores.append(f1)\n",
        "    bleu_scores.append(bleu)\n",
        "    execution_scores.append(exec_match)\n",
        "    predicted_labels.append(em)  # anggap EM sebagai label biner 1/0\n",
        "\n",
        "# ====================\n",
        "# HASIL AKHIR\n",
        "# ====================\n",
        "accuracy = accuracy_score(true_labels, predicted_labels)\n",
        "f1_binary = f1_score(true_labels, predicted_labels)\n",
        "recall = recall_score(true_labels, predicted_labels)\n",
        "\n",
        "print(\"==== EVALUASI MODEL ====\")\n",
        "print(f\"Accuracy: {accuracy:.2%}\")\n",
        "print(f\"F1 Score: {f1_binary:.2%}\")\n",
        "print(f\"Recall: {recall:.2%}\")\n",
        "print(f\"Exact Match: {sum(em_scores)/len(em_scores):.2%}\")\n",
        "print(f\"Average F1 (Token-Based): {sum(f1_scores)/len(f1_scores):.2%}\")\n",
        "print(f\"Average BLEU Score: {sum(bleu_scores)/len(bleu_scores):.2%}\")\n",
        "print(f\"Execution Match Score: {sum(execution_scores)/len(execution_scores):.2%}\")\n"
      ],
      "metadata": {
        "id": "-XPrTo_861V8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyswip\n"
      ],
      "metadata": {
        "id": "2sMGgHLrmoTW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install swi-prolog\n"
      ],
      "metadata": {
        "id": "KWgSeqrom23Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyswip import Prolog\n",
        "\n",
        "# Fungsi Exact Match (EM)\n",
        "def exact_match(predictions, references):\n",
        "    \"\"\"\n",
        "    Menghitung skor Exact Match (EM) antara prediksi dan referensi.\n",
        "    \"\"\"\n",
        "    em_score = sum([1 if pred == ref else 0 for pred, ref in zip(predictions, references)])\n",
        "    return em_score / len(references)\n",
        "\n",
        "# Fungsi Execution Match (XM) dengan PySWIP (Prolog)\n",
        "def execution_match(predictions, references):\n",
        "    \"\"\"\n",
        "    Menghitung Execution Match (XM) antara prediksi dan referensi menggunakan Prolog.\n",
        "    \"\"\"\n",
        "    prolog = Prolog()\n",
        "    match_score = 0\n",
        "\n",
        "    for pred, ref in zip(predictions, references):\n",
        "        prolog.assertz(f\"{pred}\")  # Menambahkan klausa ke Prolog\n",
        "        result = list(prolog.query(ref))  # Mengeksekusi klausa\n",
        "        if result:\n",
        "            match_score += 1\n",
        "        prolog.retractall(f\"{pred}\")  # Menghapus klausa setelah eksekusi\n",
        "\n",
        "    return match_score / len(references)\n",
        "\n",
        "# Contoh penggunaan:\n",
        "predictions = [\"penandatangan(indonesia, menteri_luar_negeri)\"]  # Pastikan tidak ada titik ganda\n",
        "references = [\"penandatangan(indonesia, menteri_luar_negeri)\"]\n",
        "\n",
        "# Menghitung metrik EM dan XM\n",
        "em_score = exact_match(predictions, references)\n",
        "xm_score = execution_match(predictions, references)\n",
        "\n",
        "print(f\"Exact Match (EM) Score: {em_score}\")\n",
        "print(f\"Execution Match (XM) Score: {xm_score}\")\n"
      ],
      "metadata": {
        "id": "W76ySXEmlZ0P"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}